<!DOCTYPE html>
<html lang="bn">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>অধ্যায় ৫: Shared Memory Tiling - CUDA GEMM টিউটোরিয়াল</title>
    <style>
        body {
            max-width: 650px;
            margin: 40px auto;
            padding: 0 20px;
            font-family: 'Kalpurush', 'Noto Sans Bengali', 'SolaimanLipi', Arial, sans-serif;
            font-size: 16px;
            line-height: 1.8;
            color: #333;
        }

        h1 {
            font-size: 32px;
            margin-bottom: 10px;
        }

        h2 {
            font-size: 24px;
            margin-top: 40px;
            margin-bottom: 20px;
        }

        h3 {
            font-size: 20px;
            margin-top: 30px;
            margin-bottom: 15px;
        }

        p {
            margin-bottom: 15px;
        }

        ul, ol {
            margin-bottom: 20px;
            padding-left: 30px;
        }

        li {
            margin-bottom: 10px;
        }

        a {
            color: #0066cc;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        code {
            background: #f4f4f4;
            padding: 2px 6px;
            font-family: 'Courier New', monospace;
            font-size: 14px;
        }

        pre {
            background: #f4f4f4;
            padding: 15px;
            overflow-x: auto;
            margin-bottom: 20px;
            border: 1px solid #ddd;
        }

        pre code {
            background: none;
            padding: 0;
        }

        .nav {
            margin-top: 50px;
            padding-top: 20px;
            border-top: 1px solid #ddd;
            display: flex;
            justify-content: space-between;
        }

        hr {
            border: none;
            border-top: 1px solid #ddd;
            margin: 40px 0;
        }

        .note {
            background: #f9f9f9;
            border-left: 3px solid #666;
            padding: 10px 15px;
            margin: 20px 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        th, td {
            border: 1px solid #ddd;
            padding: 10px;
            text-align: left;
        }

        th {
            background: #f4f4f4;
        }
    </style>
    <script src="nav.js"></script>
</head>
<body>
    <p><a href="index.html">← সূচিপত্রে ফিরে যান</a></p>

    <h1>অধ্যায় ৫: Shared Memory Tiling</h1>

    <p>এটি আমাদের সবচেয়ে গুরুত্বপূর্ণ অপটিমাইজেশন। আমরা শেয়ার্ড মেমরি (দ্রুত অন-চিপ মেমরি) ব্যবহার করব ডেটা পুনর্ব্যবহার করতে এবং গ্লোবাল মেমরি ট্রাফিক কমাতে। এটি আমাদের ~10-15x স্পিডআপ দেবে!</p>

    <h2>সমস্যা: গ্লোবাল মেমরি বটলনেক</h2>

    <p>এখন পর্যন্ত, আমরা প্রতিটি FLOP-এর জন্য গ্লোবাল মেমরি থেকে ডেটা পড়ছি:</p>

    <pre><code>for (int k = 0; k < K; k++) {
    sum += A[row * K + k] * B[k * N + col];  // প্রতি ইটারেশনে 2টি গ্লোবাল মেমরি রিড
}</code></pre>

    <p>1024×1024 ম্যাট্রিক্সের জন্য:</p>

    <ul>
        <li>প্রতিটি থ্রেড 1024 বার A এবং B থেকে পড়ে</li>
        <li>মোট 1024×1024 থ্রেড = 1M থ্রেড</li>
        <li>প্রতিটি 2048 মেমরি রিড করে = 2B মেমরি রিড!</li>
    </ul>

    <p>কিন্তু লক্ষ্য করুন: <strong>একই ডেটা বারবার পড়া হয়!</strong></p>

    <ul>
        <li>A[0][0] সারি 0-এর সব থ্রেড দ্বারা পড়া হয় (1024 বার)</li>
        <li>B[0][0] কলাম 0-এর সব থ্রেড দ্বারা পড়া হয় (1024 বার)</li>
    </ul>

    <h2>সমাধান: Tiling</h2>

    <p>মূল ধারণা: ম্যাট্রিক্সকে ছোট "টাইল" (ব্লক) এ ভাগ করুন এবং:</p>

    <ol>
        <li>একটি টাইল গ্লোবাল মেমরি থেকে দ্রুত শেয়ার্ড মেমরিতে লোড করুন</li>
        <li>ব্লকের সব থ্রেড শেয়ার্ড মেমরি থেকে এই টাইল পড়ে</li>
        <li>পরবর্তী টাইলে যান</li>
    </ol>

    <p>এটি প্রতিটি এলিমেন্ট একবার লোড করে কিন্তু TILE_SIZE বার ব্যবহার করে!</p>

    <h3>উদাহরণ: 32×32 টাইল</h3>

    <p>32×32 টাইলের জন্য:</p>

    <ul>
        <li>গ্লোবাল মেমরি লোড: 32×32 = 1024 এলিমেন্ট</li>
        <li>শেয়ার্ড মেমরি রিড: 32×32×32 = 32K রিড (প্রতিটি থ্রেড 32টি A, 32টি B পড়ে)</li>
        <li>পুনর্ব্যবহার ফ্যাক্টর: 32x!</li>
    </ul>

    <h2>টাইলিং অ্যালগরিদম</h2>

    <p>C[i][j] কম্পিউট করতে:</p>

    <pre><code>C[i][j] = Σ(k=0 to K-1) A[i][k] * B[k][j]</code></pre>

    <p>আমরা এটিকে টাইল জুড়ে বিভক্ত করি:</p>

    <pre><code>for (tile = 0; tile < K/TILE_SIZE; tile++) {
    // 1. A এবং B-র একটি টাইল শেয়ার্ডে লোড করুন
    // 2. __syncthreads() - সব থ্রেড লোডিং শেষ হওয়া পর্যন্ত অপেক্ষা করুন
    // 3. শেয়ার্ড মেমরি থেকে টাইল ব্যবহার করে আংশিক যোগফল কম্পিউট করুন
    // 4. __syncthreads() - পরবর্তী টাইল লোড করার আগে সবাই শেষ করুক
}</code></pre>

    <h2>সম্পূর্ণ কোড</h2>

    <pre><code>%%writefile gemm_shared.cu
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;cuda_runtime.h&gt;

#define TILE_SIZE 32

// Coalesced kernel (পূর্ববর্তী অধ্যায় থেকে - তুলনার জন্য)
__global__ void gemm_coalesced(int M, int N, int K, 
                               float *A, float *B_T, float *C) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (row < M && col < N) {
        float sum = 0.0f;
        for (int k = 0; k < K; k++) {
            sum += A[row * K + k] * B_T[col * K + k];
        }
        C[row * N + col] = sum;
    }
}

// Shared memory tiling kernel
__global__ void gemm_shared_mem(int M, int N, int K, 
                                float *A, float *B, float *C) {
    // শেয়ার্ড মেমরি টাইল
    __shared__ float tile_A[TILE_SIZE][TILE_SIZE];
    __shared__ float tile_B[TILE_SIZE][TILE_SIZE];
    
    // আউটপুট পজিশন
    int row = blockIdx.y * TILE_SIZE + threadIdx.y;
    int col = blockIdx.x * TILE_SIZE + threadIdx.x;
    
    float sum = 0.0f;
    
    // টাইলের উপর লুপ করুন
    int num_tiles = (K + TILE_SIZE - 1) / TILE_SIZE;
    
    for (int t = 0; t < num_tiles; t++) {
        // A থেকে টাইল লোড করুন
        int a_col = t * TILE_SIZE + threadIdx.x;
        if (row < M && a_col < K) {
            tile_A[threadIdx.y][threadIdx.x] = A[row * K + a_col];
        } else {
            tile_A[threadIdx.y][threadIdx.x] = 0.0f;
        }
        
        // B থেকে টাইল লোড করুন
        int b_row = t * TILE_SIZE + threadIdx.y;
        if (b_row < K && col < N) {
            tile_B[threadIdx.y][threadIdx.x] = B[b_row * N + col];
        } else {
            tile_B[threadIdx.y][threadIdx.x] = 0.0f;
        }
        
        // সব থ্রেড লোডিং শেষ হওয়ার জন্য অপেক্ষা করুন
        __syncthreads();
        
        // শেয়ার্ড মেমরি থেকে টাইল ব্যবহার করে আংশিক যোগফল কম্পিউট করুন
        for (int k = 0; k < TILE_SIZE; k++) {
            sum += tile_A[threadIdx.y][k] * tile_B[k][threadIdx.x];
        }
        
        // পরবর্তী টাইল লোড করার আগে সব থ্রেড শেষ হওয়ার জন্য অপেক্ষা করুন
        __syncthreads();
    }
    
    // রেজাল্ট লিখুন
    if (row < M && col < N) {
        C[row * N + col] = sum;
    }
}

// যাচাইয়ের জন্য CPU GEMM
void gemm_cpu(int M, int N, int K, 
              float *A, float *B, float *C) {
    for (int i = 0; i < M; i++) {
        for (int j = 0; j < N; j++) {
            float sum = 0.0f;
            for (int k = 0; k < K; k++) {
                sum += A[i * K + k] * B[k * N + j];
            }
            C[i * N + j] = sum;
        }
    }
}

void transpose_cpu(float *input, float *output, int rows, int cols) {
    for (int i = 0; i < rows; i++) {
        for (int j = 0; j < cols; j++) {
            output[j * rows + i] = input[i * cols + j];
        }
    }
}

bool verify_result(float *C_cpu, float *C_gpu, int M, int N) {
    for (int i = 0; i < M * N; i++) {
        if (fabs(C_cpu[i] - C_gpu[i]) > 1e-2) {
            printf("Mismatch at %d: CPU=%.6f, GPU=%.6f\n", 
                   i, C_cpu[i], C_gpu[i]);
            return false;
        }
    }
    return true;
}

float benchmark_kernel(void (*kernel)(int, int, int, float*, float*, float*),
                       int M, int N, int K, 
                       float *d_A, float *d_B, float *d_C,
                       dim3 blocks, dim3 threads) {
    cudaEvent_t start, stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);
    
    // ওয়ার্মআপ
    kernel<<<blocks, threads>>>(M, N, K, d_A, d_B, d_C);
    cudaDeviceSynchronize();
    
    // বেঞ্চমার্ক
    cudaEventRecord(start);
    for (int i = 0; i < 10; i++) {
        kernel<<<blocks, threads>>>(M, N, K, d_A, d_B, d_C);
    }
    cudaEventRecord(stop);
    cudaEventSynchronize(stop);
    
    float milliseconds = 0;
    cudaEventElapsedTime(&milliseconds, start, stop);
    
    cudaEventDestroy(start);
    cudaEventDestroy(stop);
    
    return milliseconds / 10.0f;
}

int main() {
    int M = 2048;
    int N = 2048;
    int K = 2048;
    
    printf("Matrix dimensions: C(%dx%d) = A(%dx%d) * B(%dx%d)\n\n", 
           M, N, M, K, K, N);
    
    // Host মেমরি
    size_t bytes_A = M * K * sizeof(float);
    size_t bytes_B = K * N * sizeof(float);
    size_t bytes_C = M * N * sizeof(float);
    
    float *h_A = (float*)malloc(bytes_A);
    float *h_B = (float*)malloc(bytes_B);
    float *h_B_T = (float*)malloc(bytes_B);
    float *h_C = (float*)malloc(bytes_C);
    float *h_C_ref = (float*)malloc(bytes_C);
    
    // ইনিশিয়ালাইজ
    for (int i = 0; i < M * K; i++) h_A[i] = rand() / (float)RAND_MAX;
    for (int i = 0; i < K * N; i++) h_B[i] = rand() / (float)RAND_MAX;
    transpose_cpu(h_B, h_B_T, K, N);
    
    // Device মেমরি
    float *d_A, *d_B, *d_B_T, *d_C;
    cudaMalloc(&d_A, bytes_A);
    cudaMalloc(&d_B, bytes_B);
    cudaMalloc(&d_B_T, bytes_B);
    cudaMalloc(&d_C, bytes_C);
    
    cudaMemcpy(d_A, h_A, bytes_A, cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, h_B, bytes_B, cudaMemcpyHostToDevice);
    cudaMemcpy(d_B_T, h_B_T, bytes_B, cudaMemcpyHostToDevice);
    
    // CPU রেফারেন্স
    printf("Computing CPU reference (this will take a while)...\n");
    gemm_cpu(M, N, K, h_A, h_B, h_C_ref);
    printf("CPU done.\n\n");
    
    // Coalesced kernel
    dim3 threads_coal(16, 16);
    dim3 blocks_coal((N + threads_coal.x - 1) / threads_coal.x,
                     (M + threads_coal.y - 1) / threads_coal.y);
    
    printf("=== Coalesced Kernel ===\n");
    printf("Grid: %dx%d blocks, Block: %dx%d threads\n", 
           blocks_coal.x, blocks_coal.y, threads_coal.x, threads_coal.y);
    
    float time_coal = benchmark_kernel(gemm_coalesced, M, N, K, 
                                       d_A, d_B_T, d_C, blocks_coal, threads_coal);
    cudaMemcpy(h_C, d_C, bytes_C, cudaMemcpyDeviceToHost);
    
    bool correct_coal = verify_result(h_C_ref, h_C, M, N);
    double gflops_coal = (2.0 * M * N * K) / (time_coal / 1000.0) / 1e9;
    
    printf("Result: %s\n", correct_coal ? "CORRECT" : "INCORRECT");
    printf("Time: %.3f ms\n", time_coal);
    printf("GFLOPS: %.2f\n\n", gflops_coal);
    
    // Shared memory kernel
    dim3 threads_shared(TILE_SIZE, TILE_SIZE);
    dim3 blocks_shared((N + TILE_SIZE - 1) / TILE_SIZE,
                       (M + TILE_SIZE - 1) / TILE_SIZE);
    
    printf("=== Shared Memory Tiling Kernel ===\n");
    printf("Tile size: %dx%d\n", TILE_SIZE, TILE_SIZE);
    printf("Grid: %dx%d blocks, Block: %dx%d threads\n", 
           blocks_shared.x, blocks_shared.y, threads_shared.x, threads_shared.y);
    
    float time_shared = benchmark_kernel(gemm_shared_mem, M, N, K, 
                                         d_A, d_B, d_C, blocks_shared, threads_shared);
    cudaMemcpy(h_C, d_C, bytes_C, cudaMemcpyDeviceToHost);
    
    bool correct_shared = verify_result(h_C_ref, h_C, M, N);
    double gflops_shared = (2.0 * M * N * K) / (time_shared / 1000.0) / 1e9;
    
    printf("Result: %s\n", correct_shared ? "CORRECT" : "INCORRECT");
    printf("Time: %.3f ms\n", time_shared);
    printf("GFLOPS: %.2f\n\n", gflops_shared);
    
    // তুলনা
    printf("=== Performance Comparison ===\n");
    printf("Speedup vs coalesced: %.2fx\n", time_coal / time_shared);
    printf("GFLOPS: %.2f → %.2f (%.1f%% increase)\n",
           gflops_coal, gflops_shared, 
           (gflops_shared - gflops_coal) / gflops_coal * 100);
    printf("Peak utilization: %.1f%%\n", (gflops_shared / 8100.0) * 100);
    
    // ক্লিনআপ
    free(h_A); free(h_B); free(h_B_T); free(h_C); free(h_C_ref);
    cudaFree(d_A); cudaFree(d_B); cudaFree(d_B_T); cudaFree(d_C);
    
    return 0;
}</code></pre>

    <p>কম্পাইল এবং রান করুন:</p>

    <pre><code>!nvcc gemm_shared.cu -o gemm_shared
!./gemm_shared</code></pre>

    <h2>প্রত্যাশিত আউটপুট</h2>

    <p>2048×2048 ম্যাট্রিক্সের জন্য T4-তে:</p>

    <pre><code>Matrix dimensions: C(2048x2048) = A(2048x2048) * B(2048x2048)

Computing CPU reference (this will take a while)...
CPU done.

=== Coalesced Kernel ===
Grid: 128x128 blocks, Block: 16x16 threads
Result: CORRECT
Time: 48.xxx ms
GFLOPS: ~360

=== Shared Memory Tiling Kernel ===
Tile size: 32x32
Grid: 64x64 blocks, Block: 32x32 threads
Result: CORRECT
Time: 3.8 ms
GFLOPS: ~4500

=== Performance Comparison ===
Speedup vs coalesced: 12.6x
GFLOPS: 360 → 4500 (1150% increase)
Peak utilization: 55.6%</code></pre>

    <h2>কোড বুঝে নেওয়া</h2>

    <h3>শেয়ার্ড মেমরি ডিক্লারেশন</h3>

    <pre><code>__shared__ float tile_A[TILE_SIZE][TILE_SIZE];
__shared__ float tile_B[TILE_SIZE][TILE_SIZE];</code></pre>

    <p><code>__shared__</code> কীওয়ার্ড মেমরি ব্লকের সব থ্রেড দ্বারা শেয়ার করা হয়। প্রতিটি ব্লকের নিজস্ব কপি আছে।</p>

    <h3>টাইল লোডিং</h3>

    <pre><code>int a_col = t * TILE_SIZE + threadIdx.x;
if (row < M && a_col < K) {
    tile_A[threadIdx.y][threadIdx.x] = A[row * K + a_col];
} else {
    tile_A[threadIdx.y][threadIdx.x] = 0.0f;
}</code></pre>

    <p>প্রতিটি থ্রেড টাইলের একটি এলিমেন্ট লোড করে। 32×32 থ্রেড = 32×32 এলিমেন্ট লোড করা হয়। বাউন্ডের বাইরে: শূন্য প্যাড করা হয়।</p>

    <h3>Synchronization</h3>

    <pre><code>__syncthreads();</code></pre>

    <p>এটি অত্যন্ত গুরুত্বপূর্ণ! এটি নিশ্চিত করে:</p>

    <ul>
        <li>ব্যবহার করার আগে সব থ্রেড তাদের ডেটা শেয়ার্ডে লোড শেষ করেছে</li>
        <li>পরবর্তী টাইল লোড করার আগে সব থ্রেড বর্তমান টাইল ব্যবহার শেষ করেছে</li>
    </ul>

    <p>যদি আপনি <code>__syncthreads()</code> ভুলে যান, আপনি ভুল ফলাফল পাবেন!</p>

    <h3>কম্পিউটেশন</h3>

    <pre><code>for (int k = 0; k < TILE_SIZE; k++) {
    sum += tile_A[threadIdx.y][k] * tile_B[k][threadIdx.x];
}</code></pre>

    <p>এখন আমরা দ্রুত শেয়ার্ড মেমরি থেকে পড়ছি, গ্লোবাল মেমরি নয়!</p>

    <h2>মেমরি ট্রাফিক বিশ্লেষণ</h2>

    <p>2048×2048 ম্যাট্রিক্সের জন্য তুলনা করুন:</p>

    <h3>Coalesced (কোন শেয়ার্ড মেমরি নেই):</h3>

    <ul>
        <li>প্রতিটি থ্রেড A এবং B থেকে 2048 বার পড়ে</li>
        <li>মোট: 2048×2048 থ্রেড × 2048 রিড × 2 ম্যাট্রিক্স × 4 বাইট</li>
        <li>= 68 GB গ্লোবাল মেমরি রিড</li>
    </ul>

    <h3>Shared Memory Tiling:</h3>

    <ul>
        <li>টাইল লোড: প্রতিটি এলিমেন্ট একবার গ্লোবাল থেকে শেয়ার্ডে</li>
        <li>মোট: 2 ম্যাট্রিক্স × 2048×2048 এলিমেন্ট × 4 বাইট</li>
        <li>= 67 MB গ্লোবাল মেমরি রিড (শুধুমাত্র!)</li>
        <li>বাকি সব দ্রুত শেয়ার্ড মেমরি থেকে</li>
    </ul>

    <p><strong>1000x কম গ্লোবাল মেমরি ট্রাফিক!</strong></p>

    <h2>কেন এটি দ্রুত?</h2>

    <ol>
        <li><strong>ডেটা পুনর্ব্যবহার:</strong> প্রতিটি এলিমেন্ট একবার লোড করা হয়, TILE_SIZE বার ব্যবহার করা হয়</li>
        <li><strong>দ্রুত মেমরি:</strong> শেয়ার্ড মেমরি ~100x গ্লোবাল মেমরির চেয়ে দ্রুত</li>
        <li><strong>উচ্চ arithmetic intensity:</strong> অনেক কম্পিউটেশন প্রতি মেমরি এক্সেস</li>
        <li><strong>ব্যান্ডউইথ সেভিং:</strong> গ্লোবাল মেমরি বটলনেক সরানো হয়েছে</li>
    </ol>

    <h2>শেয়ার্ড মেমরি ব্যাংক কনফ্লিক্ট</h2>

    <p>শেয়ার্ড মেমরি 32টি "ব্যাংক"-এ সংগঠিত। যদি একটি ওয়ার্পের একাধিক থ্রেড একই ব্যাংক এক্সেস করে, এটি সিরিয়ালাইজ হয় (ধীর)।</p>

    <p>আমাদের কোডে:</p>

    <ul>
        <li><code>tile_A[threadIdx.y][k]</code> - বিভিন্ন সারি, কোন কনফ্লিক্ট নেই</li>
        <li><code>tile_B[k][threadIdx.x]</code> - পাশের কলাম, কোন কনফ্লিক্ট নেই</li>
    </ul>

    <p>আমরা ভাগ্যবান - কোন ব্যাংক কনফ্লিক্ট নেই! পরবর্তী অধ্যায়ে আমরা এটি আরো সতর্কভাবে হ্যান্ডেল করব।</p>

    <h2>বিভিন্ন টাইল সাইজ</h2>

    <p>TILE_SIZE পরিবর্তন করে দেখুন:</p>

    <table